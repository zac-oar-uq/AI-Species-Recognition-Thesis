{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('myenv': conda)"
  },
  "interpreter": {
   "hash": "1f584d1f2099a3b6b1aabed6fa6c4f9531bcc1f97f929a8d4f3fbf52265911e6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "from tensorflow.keras.applications import InceptionV3\r\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\r\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from tensorflow import keras\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.image as img\r\n",
    "import numpy as np\r\n",
    "from os import listdir\r\n",
    "from os.path import isfile\r\n",
    "import math\r\n",
    "\r\n",
    "from datetime import datetime\r\n",
    "%load_ext tensorboard"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "class SequenceGenerator(keras.utils.Sequence):\r\n",
    "    \"\"\"\r\n",
    "    A keras Sequence to be used as an image generator for the model.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, x, y, batchsize):\r\n",
    "        self.x, self.y, self.batchsize = x, y, batchsize\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return math.ceil(len(self.x) / self.batchsize)\r\n",
    "\r\n",
    "    def names_at_batch(self, idx):\r\n",
    "        x_names = self.x[idx * self.batchsize:(idx + 1) * self.batchsize]\r\n",
    "        y_names = np.asarray(self.y[idx * self.batchsize:(idx + 1) * self.batchsize])\r\n",
    "        return x_names, y_names\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        x_names = self.x[idx * self.batchsize:(idx + 1) * self.batchsize]\r\n",
    "        y_names = np.asarray(self.y[idx * self.batchsize:(idx + 1) * self.batchsize])\r\n",
    "\r\n",
    "        # open x image names, resize, normalise and make a numpy array\r\n",
    "        x1 = np.asarray([preprocess_input(img_to_array(load_img(file_name[0], target_size=(299, 299)))) for file_name in x_names])\r\n",
    "        x2 = np.asarray([preprocess_input(img_to_array(load_img(file_name[1], target_size=(299, 299)))) for file_name in x_names])\r\n",
    "        x3 = np.asarray([preprocess_input(img_to_array(load_img(file_name[2], target_size=(299, 299)))) for file_name in x_names])\r\n",
    "        # x1 = np.asarray([img_to_array(load_img(file_name[0], target_size=(299, 299))) for file_name in x_names]) / 255.0\r\n",
    "        # x2 = np.asarray([img_to_array(load_img(file_name[1], target_size=(299, 299))) for file_name in x_names]) / 255.0\r\n",
    "        # x3 = np.asarray([img_to_array(load_img(file_name[2], target_size=(299, 299))) for file_name in x_names]) / 255.0\r\n",
    "\r\n",
    "        return [x1, x2, x3], y_names\r\n",
    "\r\n",
    "    def num_classes(self):\r\n",
    "        ret = []\r\n",
    "        for cat in self.y:\r\n",
    "            if cat not in ret:\r\n",
    "                ret.append(cat)\r\n",
    "        return len(ret)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "def shifted_arr(arr, shift):\r\n",
    "    new_arr = [None, None, None]\r\n",
    "    for i in range(3):\r\n",
    "        new_arr[i] = arr[(i + shift) % 3][0]\r\n",
    "    \r\n",
    "    return new_arr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "def get_filenames_from_dir(directory):\r\n",
    "    x = []\r\n",
    "    y = []\r\n",
    "    category_count = 0\r\n",
    "    for category in listdir(directory):\r\n",
    "        triplet = []\r\n",
    "        if isfile(category):\r\n",
    "            continue\r\n",
    "        for file in listdir(\"{0}/{1}\".format(directory, category)):\r\n",
    "            if file[-3:] != \"jpg\":\r\n",
    "                continue\r\n",
    "            triplet.append((\"{0}/{1}/{2}\".format(directory, category, file), category_count))\r\n",
    "            if len(triplet) == 3:\r\n",
    "                for i in range(3):\r\n",
    "                    x.append(shifted_arr(triplet, i))\r\n",
    "                    y.append(triplet[0][1])\r\n",
    "                triplet = []\r\n",
    "        category_count += 1\r\n",
    "\r\n",
    "    return x, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# makes arrays of the images and label names\r\n",
    "x_names, y_names = get_filenames_from_dir(\"database\")\r\n",
    "\r\n",
    "# 15% of all the images are set aside as the test set\r\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(x_names, y_names, test_size=0.15, random_state=42)\r\n",
    "\r\n",
    "# 17% of the non-test images are set aside as the validation set\r\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.17, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# make generators with batch size 32 for each set\r\n",
    "train_gen = SequenceGenerator(x_train, y_train, 4)\r\n",
    "val_gen = SequenceGenerator(x_val, y_val, 4)\r\n",
    "test_gen = SequenceGenerator(x_test, y_test, 4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# Model 1\r\n",
    "base_model1 = InceptionV3(weights=\"imagenet\")\r\n",
    "for layer in base_model1.layers:\r\n",
    "    layer._name += \"_1\"\r\n",
    "    layer.trainable = True\r\n",
    "\r\n",
    "# Model 2\r\n",
    "base_model2 = InceptionV3(weights=\"imagenet\")\r\n",
    "for layer in base_model2.layers:\r\n",
    "    layer._name += \"_2\"\r\n",
    "    layer.trainable = True\r\n",
    "\r\n",
    "# Model 3\r\n",
    "base_model3 = InceptionV3(weights=\"imagenet\")\r\n",
    "for layer in base_model3.layers:\r\n",
    "    layer._name += \"_3\"\r\n",
    "    layer.trainable = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "out_layer = keras.layers.concatenate([base_model1.layers[-2].output, base_model2.layers[-2].output, base_model3.layers[-2].output])\r\n",
    "predictions = keras.layers.Dense(291, activation='softmax')(out_layer)\r\n",
    "\r\n",
    "model = keras.Model(inputs=[base_model1.input, base_model2.input, base_model3.input], outputs=predictions)\r\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "logdir = \"logs/3model_ensemb_shared_concat\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "model.fit(train_gen, validation_data=val_gen, callbacks=[tensorboard_callback], epochs=20)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "11126/11126 [==============================] - 2465s 219ms/step - loss: 4.9040 - accuracy: 0.0421 - val_loss: 23.1668 - val_accuracy: 0.0449\n",
      "Epoch 2/20\n",
      "11126/11126 [==============================] - 2391s 215ms/step - loss: 3.3397 - accuracy: 0.1981 - val_loss: 2.5583 - val_accuracy: 0.3140\n",
      "Epoch 3/20\n",
      "11126/11126 [==============================] - 2356s 212ms/step - loss: 1.9375 - accuracy: 0.4525 - val_loss: 1.6437 - val_accuracy: 0.5166\n",
      "Epoch 4/20\n",
      "11126/11126 [==============================] - 2368s 213ms/step - loss: 1.3185 - accuracy: 0.6005 - val_loss: 1.4488 - val_accuracy: 0.5744\n",
      "Epoch 5/20\n",
      "11126/11126 [==============================] - 2353s 211ms/step - loss: 0.9811 - accuracy: 0.6932 - val_loss: 1.2140 - val_accuracy: 0.6567\n",
      "Epoch 6/20\n",
      "11126/11126 [==============================] - 2371s 213ms/step - loss: 0.7695 - accuracy: 0.7557 - val_loss: 0.9501 - val_accuracy: 0.7262\n",
      "Epoch 7/20\n",
      "11126/11126 [==============================] - 2366s 213ms/step - loss: 0.6245 - accuracy: 0.7981 - val_loss: 0.8514 - val_accuracy: 0.7566\n",
      "Epoch 8/20\n",
      "11126/11126 [==============================] - 2369s 213ms/step - loss: 0.5044 - accuracy: 0.8334 - val_loss: 1.3173 - val_accuracy: 0.6519\n",
      "Epoch 9/20\n",
      "11126/11126 [==============================] - 2372s 213ms/step - loss: 0.4113 - accuracy: 0.8620 - val_loss: 0.7783 - val_accuracy: 0.7694\n",
      "Epoch 10/20\n",
      "11126/11126 [==============================] - 2373s 213ms/step - loss: 0.3362 - accuracy: 0.8881 - val_loss: 0.6605 - val_accuracy: 0.8079\n",
      "Epoch 11/20\n",
      "11126/11126 [==============================] - 2360s 212ms/step - loss: 0.2841 - accuracy: 0.9042 - val_loss: 0.6260 - val_accuracy: 0.8217\n",
      "Epoch 12/20\n",
      "11126/11126 [==============================] - 2246s 202ms/step - loss: 0.2401 - accuracy: 0.9180 - val_loss: 0.9111 - val_accuracy: 0.7995\n",
      "Epoch 13/20\n",
      "11126/11126 [==============================] - 2216s 199ms/step - loss: 0.1950 - accuracy: 0.9340 - val_loss: 0.8292 - val_accuracy: 0.8000\n",
      "Epoch 14/20\n",
      "11126/11126 [==============================] - 2210s 199ms/step - loss: 0.1640 - accuracy: 0.9441 - val_loss: 2.0023 - val_accuracy: 0.7838\n",
      "Epoch 15/20\n",
      "11126/11126 [==============================] - 2213s 199ms/step - loss: 0.1393 - accuracy: 0.9526 - val_loss: 1.4476 - val_accuracy: 0.8133\n",
      "Epoch 16/20\n",
      "11126/11126 [==============================] - 2203s 198ms/step - loss: 0.1229 - accuracy: 0.9577 - val_loss: 0.7322 - val_accuracy: 0.8270\n",
      "Epoch 17/20\n",
      "11126/11126 [==============================] - 2208s 198ms/step - loss: 0.1093 - accuracy: 0.9640 - val_loss: 1.3217 - val_accuracy: 0.7732\n",
      "Epoch 18/20\n",
      "11126/11126 [==============================] - 2197s 197ms/step - loss: 0.1033 - accuracy: 0.9652 - val_loss: 0.7387 - val_accuracy: 0.8563\n",
      "Epoch 19/20\n",
      "11126/11126 [==============================] - 2173s 195ms/step - loss: 0.0898 - accuracy: 0.9685 - val_loss: 1.9442 - val_accuracy: 0.8090\n",
      "Epoch 20/20\n",
      "11126/11126 [==============================] - 2187s 197ms/step - loss: 0.0824 - accuracy: 0.9725 - val_loss: 0.7913 - val_accuracy: 0.8253\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d178dd7070>"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "model.evaluate(test_gen)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2366/2366 [==============================] - 154s 65ms/step - loss: 0.8073 - accuracy: 0.8217\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.8073202967643738, 0.8217079043388367]"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  }
 ]
}